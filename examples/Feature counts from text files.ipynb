{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 474,
   "id": "afraid-listening",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For development, use local paths.\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "id": "copyrighted-leader",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# Load local\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "id": "lesser-neighborhood",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nonconsumptive as nc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-stream",
   "metadata": {},
   "source": [
    "# Feature counts from text files.\n",
    "\n",
    "This notebook creating a set of document-level feature count files akin to those distributed by the Hathi Trust, but from Project Gutenberg text files in the folder `{nonconsumptive_root}/sample_inputs/gutenberg/texts`.\n",
    "Metadata is read from a file called \"metadata.json\" and bound to files based on their filenames.\n",
    "\n",
    "Files are stored as parquet, which allows for fast processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optimum-klein",
   "metadata": {},
   "source": [
    "# Create a corpus\n",
    "\n",
    "First, create a corpus. Every corpus has to be build from a strategy for retrieving texts, and a strategy for retrieving metadata.\n",
    "\n",
    "Ideally these will be disentangled. Some strategies might include:\n",
    "\n",
    "* metadata from { csv, yaml header block, TEI header blocks }\n",
    "* text from { set of files }\n",
    "* ids from { filename, filename plus directory, first column of mallet input,  etc. }\n",
    "\n",
    "The ids allow looking up the texts in the metadata.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "id": "processed-accommodation",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg = nc.Corpus(\"../sample_inputs/gutenberg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equivalent-violation",
   "metadata": {},
   "source": [
    "## Metadata\n",
    "\n",
    "The metadata is stored internally as a pyarrow table with some wrappers to ensure type integrity.\n",
    "\n",
    "Based on internal data and column types, this will leverage some Bookworm code to determine that \"date\"  or \"year\" are date type columns.\n",
    "It should also be able to discriminate between \"categorical\" types (or in library parlance, \"controlled vocabulary\" fields and free entry ones, perhaps with additional help from configuration files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "id": "fifteen-tracker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>htid</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>title</th>\n",
       "      <th>author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15</td>\n",
       "      <td>dul1.ark+=13960=t3kw6ns1s</td>\n",
       "      <td>1851</td>\n",
       "      <td>Moby-Dick; or, The Whale</td>\n",
       "      <td>Melville, Herman</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27</td>\n",
       "      <td>coo.31924014152700</td>\n",
       "      <td>1894</td>\n",
       "      <td>Far from the Madding Crowd</td>\n",
       "      <td>Hardy, Thomas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60</td>\n",
       "      <td>nyp.33433075744890</td>\n",
       "      <td>1905</td>\n",
       "      <td>The Scarlet Pimpernel</td>\n",
       "      <td>Orczy, Emmuska Orczy, Baroness</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>62</td>\n",
       "      <td>hvd.32044004480208</td>\n",
       "      <td>1917</td>\n",
       "      <td>A Princess of Mars</td>\n",
       "      <td>Burroughs, Edgar Rice</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                       htid  pubdate                       title  \\\n",
       "0  15  dul1.ark+=13960=t3kw6ns1s     1851    Moby-Dick; or, The Whale   \n",
       "1  27         coo.31924014152700     1894  Far from the Madding Crowd   \n",
       "2  60         nyp.33433075744890     1905       The Scarlet Pimpernel   \n",
       "3  62         hvd.32044004480208     1917          A Princess of Mars   \n",
       "\n",
       "                           author  \n",
       "0                Melville, Herman  \n",
       "1                   Hardy, Thomas  \n",
       "2  Orczy, Emmuska Orczy, Baroness  \n",
       "3           Burroughs, Edgar Rice  "
      ]
     },
     "execution_count": 479,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.metadata.tb.to_pandas().head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-spice",
   "metadata": {},
   "source": [
    "Individual entries can be retrieved by their identifier. The identifier field should be called 'filename' or 'id,' or (ultimately) specified in the definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "blond-future",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '179',\n",
       " 'htid': 'uc2.ark+=13960=t84j0c38h',\n",
       " 'pubdate': 1879,\n",
       " 'title': 'The Europeans',\n",
       " 'author': 'James, Henry'}"
      ]
     },
     "execution_count": 597,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.metadata.get(\"179\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reverse-groove",
   "metadata": {},
   "source": [
    "# Documents\n",
    "\n",
    "The documents part of the corpus is structured as an iterator, because it's generally foolhardy to read in all the documents at once.\n",
    "\n",
    "Right now, the text of the document is read at iteration. Ultimately, the strategy would be to read only the parts of the document \n",
    "as needed from the corpus. (For example, if you request feature counts, it's fine if the raw document isn't there.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "signed-segment",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method Corpus.documents of <nonconsumptive.corpus.Corpus object at 0x7fd7418c0970>>"
      ]
     },
     "execution_count": 598,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.documents # Is an iterable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "victorian-effectiveness",
   "metadata": {},
   "source": [
    "## An individual document\n",
    "\n",
    "`first` is a convenience method to get a single document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "focal-litigation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265775"
      ]
     },
     "execution_count": 599,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_book = gutenberg.first()\n",
    "len(one_book.tokenize())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "linear-distance",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "passing-sucking",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../sample_inputs/gutenberg/tokenized\n"
     ]
    }
   ],
   "source": [
    "z = gutenberg.tokenizer.get_tokens(\"15\").value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "earned-course",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../sample_inputs/gutenberg/tokenized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.RecordBatch\n",
       "token: string\n",
       "count: uint32"
      ]
     },
     "execution_count": 596,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.first().wordcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-diameter",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "awful-institute",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../sample_inputs/gutenberg/tokenized\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pyarrow.RecordBatch\n",
       "token: string\n",
       "count: int64"
      ]
     },
     "execution_count": 608,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = gutenberg.tokenizer.get_tokens(\"15\")\n",
    "c = pa.RecordBatch.from_struct_array(tokens.value_counts())\n",
    "pa.record_batch(\n",
    "   [c['values'],c['counts']], names = ['token', 'count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smaller-atlas",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "comic-foster",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      12      12     540\n"
     ]
    }
   ],
   "source": [
    "ls ../sample_inputs/gutenberg/tokenized | wc "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "several-reflection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '15',\n",
       " 'htid': 'dul1.ark+=13960=t3kw6ns1s',\n",
       " 'pubdate': 1851,\n",
       " 'title': 'Moby-Dick; or, The Whale',\n",
       " 'author': 'Melville, Herman'}"
      ]
     },
     "execution_count": 610,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_book.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "id": "accessory-angola",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "e king.” —_Blackstone_.\n",
      "\n",
      "“Soon to the sport of death the crews repair:\n",
      "Rodmond unerring o’er his head suspends\n",
      "The barbed steel, and every turn attends.”\n",
      "—_Falconer’s Shipwreck_.\n",
      "\n",
      "“Bright shone the roofs, the domes, the spires,\n",
      "    And rockets blew s\n"
     ]
    }
   ],
   "source": [
    "print(one_book.full_text[17000:17250])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "id": "buried-mattress",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.—', 'The', 'Chase', '.', 'First', 'Day', 'CHAPTER', 'CXXXV', '.—', 'The']"
      ]
     },
     "execution_count": 434,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_book.tokens[1000:1010]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "inner-advice",
   "metadata": {},
   "source": [
    "## Wordcounts\n",
    "\n",
    "Wordcounts are a basic element of nonconsumptive reading that can be used in analysis or stored. They are returned as a pyarrow RecordBatch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "id": "fuzzy-outdoors",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>﻿</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56837</th>\n",
       "      <td>64</td>\n",
       "      <td>golden</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56836</th>\n",
       "      <td>64</td>\n",
       "      <td>mystical</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56835</th>\n",
       "      <td>64</td>\n",
       "      <td>animal</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56834</th>\n",
       "      <td>64</td>\n",
       "      <td>Human</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74947</th>\n",
       "      <td>85</td>\n",
       "      <td>,</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20736</th>\n",
       "      <td>24</td>\n",
       "      <td>,</td>\n",
       "      <td>211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70633</th>\n",
       "      <td>80</td>\n",
       "      <td>,</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78423</th>\n",
       "      <td>89</td>\n",
       "      <td>,</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30304</th>\n",
       "      <td>35</td>\n",
       "      <td>,</td>\n",
       "      <td>222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>93595 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       chunk     token  count\n",
       "0          0         ﻿      1\n",
       "56837     64    golden      1\n",
       "56836     64  mystical      1\n",
       "56835     64    animal      1\n",
       "56834     64     Human      1\n",
       "...      ...       ...    ...\n",
       "74947     85         ,    211\n",
       "20736     24         ,    211\n",
       "70633     80         ,    214\n",
       "78423     89         ,    217\n",
       "30304     35         ,    222\n",
       "\n",
       "[93595 rows x 3 columns]"
      ]
     },
     "execution_count": 472,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_book.chunked_wordcounts(2500).to_pandas().sort_values(\"count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "id": "experienced-essay",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>token</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>500</th>\n",
       "      <td>whale</td>\n",
       "      <td>914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>598</th>\n",
       "      <td>whales</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1413</th>\n",
       "      <td>whale_</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1829</th>\n",
       "      <td>whalemen</td>\n",
       "      <td>70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2443</th>\n",
       "      <td>whaling</td>\n",
       "      <td>118</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         token  count\n",
       "500      whale    914\n",
       "598     whales    239\n",
       "1413    whale_      3\n",
       "1829  whalemen     70\n",
       "2443   whaling    118"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_book.wordcounts.to_pandas().query(\"token.str.match('whal')\").head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "former-election",
   "metadata": {},
   "source": [
    "## Metadata on wordcounts\n",
    "\n",
    "The schema includes metadata. Figuring out how to dress this up into full json-ld  is a major goal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "id": "foster-break",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{b'nc_metadata': b'{\"id\": \"15\", \"htid\": \"dul1.ark+=13960=t3kw6ns1s\", \"pubdate\": 1851, \"title\": \"Moby-Dick; or, The Whale\", \"author\": \"Melville, Herman\"}'}"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_book.wordcounts.schema.metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "informative-venue",
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg.write_feature_counts(\"../sample_inputs/gutenberg/feature_counts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "pursuant-collectible",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103.parquet 141.parquet 165.parquet 178.parquet 27.parquet  84.parquet\n",
      "105.parquet 142.parquet 170.parquet 179.parquet 60.parquet  86.parquet\n",
      "119.parquet 144.parquet 171.parquet 203.parquet 62.parquet  91.parquet\n",
      "121.parquet 145.parquet 172.parquet 215.parquet 64.parquet  94.parquet\n",
      "126.parquet 15.parquet  173.parquet 217.parquet 72.parquet  95.parquet\n",
      "133.parquet 155.parquet 174.parquet 222.parquet 73.parquet\n",
      "135.parquet 158.parquet 175.parquet 224.parquet 76.parquet\n",
      "139.parquet 161.parquet 176.parquet 233.parquet 78.parquet\n",
      "140.parquet 164.parquet 177.parquet 234.parquet 82.parquet\n"
     ]
    }
   ],
   "source": [
    "!ls ../sample_inputs/gutenberg/feature_counts/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-manner",
   "metadata": {},
   "source": [
    "## Iterating over feature counts\n",
    "\n",
    "We can now iterate over the token count files to get a list of--say--which books use words the most."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "id": "modified-drilling",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(914, 'Moby-Dick; or, The Whale', 265775),\n",
       " (34, 'Twenty Thousand Leagues under the Sea', 127587),\n",
       " (3, 'Frankenstein; Or, The Modern Prometheus', 89394),\n",
       " (1, 'The Red Badge of Courage: An Episode of the American Civil War', 59152),\n",
       " (1, 'The Poison Belt', 38688),\n",
       " (1, 'The Lost World', 93809),\n",
       " (1, 'The Call of the Wild', 41323),\n",
       " (1, 'McTeague: A Story of San Francisco', 146942),\n",
       " (1, 'Les Misérables', 689604),\n",
       " (1, 'Adventures of Huckleberry Finn', 146747)]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whales = []\n",
    "\n",
    "for meta, counts in gutenberg.feature_counts(\"../sample_inputs/gutenberg/feature_counts/\"):\n",
    "    words = counts.to_pandas()['count'].sum()\n",
    "    title = meta['title']\n",
    "    whale_counts = counts.to_pandas().query(\"token=='whale'\")['count'].sum()\n",
    "    whales.append((whale_counts, title, words))\n",
    "\n",
    "whales.sort(reverse = True)\n",
    "whales[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "signed-exhibit",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.pre_tokenize_str(doc.fulltext[:1000])\n",
    "\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, decoders, trainers\n",
    "\n",
    "tokenizer = Tokenizer(models.WordLevel())\n",
    "tokenizer.normalizer = normalizers.NFKC()\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Sequence([UnicodeScripts(), Whitespace()])\n",
    "tokenizer.decoders = decoders.ByteLevel()\n",
    "\n",
    "trainer = trainers.WordLevelTrainer(\n",
    "    show_progress=True,\n",
    "    vocab_size=1_000_000,\n",
    ")\n",
    "\n",
    "tokenizer.train([str(f) for f in files], trainer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "departmental-committee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../sample_inputs/gutenberg/texts/119.txt\r"
     ]
    }
   ],
   "source": [
    "for file in gutenberg.files():\n",
    "    print(file, end = \"\\r\")\n",
    "    tokenized = tokenizer.pre_tokenizer.pre_tokenize_str(file.open().read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "funky-tournament",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyarrow.lib.StringArray object at 0x7fd75224a760>\n",
       "[\n",
       "  \"﻿\",\n",
       "  \"The\",\n",
       "  \"Project\",\n",
       "  \"Gutenberg\",\n",
       "  \"EBook\",\n",
       "  \"of\",\n",
       "  \"A\",\n",
       "  \"Tramp\",\n",
       "  \"Abroad\",\n",
       "  \",\",\n",
       "  ...\n",
       "  \"to\",\n",
       "  \"our\",\n",
       "  \"email\",\n",
       "  \"newsletter\",\n",
       "  \"to\",\n",
       "  \"hear\",\n",
       "  \"about\",\n",
       "  \"new\",\n",
       "  \"eBooks\",\n",
       "  \".\"\n",
       "]"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pa.array([t[0] for t in tokenized])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cognitive-taiwan",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.save(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "facial-ridge",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.encode(doc.fulltext[:1000]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "chronic-activation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ufeff The Project Gutenberg EBook of Confidence , by Henry James This eBook is for the use of anyone anywhere at no cost and with almost no restrictions whatsoever . You may copy it , give it away or re - use it under the terms of the Project Gutenberg License included with this eBook or online at www . gutenberg . org Title : Confidence Author : Henry James Release Date : March 14 , 2006 [ EBook # 178 ] Last Updated : September 18 , 2016 Language : English Character set encoding : UTF - 8 *** START OF THIS PROJECT GUTENBERG EBOOK CONFIDENCE *** Produced by Judith Boss and David Widger CONFIDENCE by Henry James CHAPTER I It was in the early days of April ; Bernard Longueville had been spending the winter in Rome . He had travelled northward with the consciousness of several social duties that appealed to him from the further side of the Alps , but he was under the charm of the Italian spring , and he made a pretext for lingering . He had spent five days at Siena , where he had intended to spen'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(doc.fulltext[:1000]).ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spare-enhancement",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tf]",
   "language": "python",
   "name": "conda-env-tf-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
